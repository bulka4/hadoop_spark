#!/bin/bash

# That script will be executed on a VM as a root user. It will be at first rendered using the Terraform templatefile function.
# We are using here variables provided by that function:
# - username
# - hostnames
# - host_entries
# - ssh_public_key


hostnames=( %{ for entry in hostnames ~} "${entry}" %{ endfor ~} )
hadoop_master_hostname=$${hostnames[0]}
hadoop_slave_hostname=$${hostnames[1]}


# === Section 1: Assign given hostnames to the private IP addresses of both VMs. ===

HOSTS_FILE="/etc/hosts"

echo "Adding entries to $HOSTS_FILE..."

# Entries to add to the /etc/hosts, that is lines mapping hostnames to IP addresses.
hosts_entries=( %{ for entry in host_entries ~} "${entry}" %{ endfor ~} )

for entry in "$${hosts_entries[@]}"; do
  # Check if entry already exists
  if grep -q "$entry" "$HOSTS_FILE"; then
    echo "Entry '$entry' already exists. Skipping."
  else
    # add entry to the hosts file
    echo "$entry" >> "$HOSTS_FILE"
    echo "Added: $entry"
  fi
done

echo "Done."



# === Section 2: Adding a SSH public key to the authorized keys so Hadoop (and we) can connect from VM1. ===

mkdir -p /home/${username}/.ssh # Create the .ssh directory if it doesn't exists
chmod 700 /home/${username}/.ssh # Set correct permissions

AUTHORIZED_KEYS="/home/${username}/.ssh/authorized_keys"

echo "${ssh_public_key}" >> "$AUTHORIZED_KEYS" # Add the SSH public key to the authorized keys.

chmod 600 "$AUTHORIZED_KEYS" # Set correct permissions for the file with authorized keys.
chown -R ${username}:${username} /home/${username}/.ssh # Assign the ${username} user as the owner of the .ssh folder.

echo "SSH public key added to authorized_keys."


# === Section 3: Install Java ===

apt-get update
apt-get install openjdk-8-jdk -y


# === Section 4: Download Hadoop and Spark. Save both in the /home/${username}/hadoop and /home/${username}/spark respectively. ===

apt-get install curl
curl -L https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz | tar -xz -C /home/${username}
mv /home/${username}/hadoop-3.3.6 /home/${username}/hadoop

curl -L https://archive.apache.org/dist/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz | tar -xz -C /home/${username}
mv /home/${username}/spark-3.4.4-bin-hadoop3 /home/${username}/spark


# === Section 5: Modify the .bashrc file. We will define there environment variables needed for Hadoop. ===

# add all the below lines to the .bashrc file
cat << 'EOF' >> /home/${username}/.bashrc
export HADOOP_HOME="/home/${username}/hadoop"
export SPARK_HOME="/home/${username}/spark"
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export PATH=$PATH:$SPARK_HOME/bin
export MAPRED_HOME=$${HADOOP_HOME}
export HDFS_HOME=$${HADOOP_HOME}
EOF


# === Section 6: Modify the hadoop-env.sh, yarn-env.sh and spark-env.sh files. Add value for the JAVA_HOME environment variable. ===

echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" >> /home/${username}/hadoop/etc/hadoop/hadoop-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" >> /home/${username}/hadoop/etc/hadoop/yarn-env.sh

cp /home/${username}/spark/conf/spark-env.sh.template /home/${username}/spark/conf/spark-env.sh
echo "export SPARK_WORKER_CORES=12" >> /home/${username}/spark/conf/spark-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" >> /home/${username}/spark/conf/spark-env.sh


# === Section 7: Modify the core-site.xml file ===

# add all the below properties to the configuration section of the core-site.xml file

# We are using here syntax: 
# > sed -i '/pattern/i sentence' file_path
# We have here 3 arguments: 'pattern', 'sentence' and 'file_path'.
# It is finding a string matching the 'pattern' argument in the file at the path specified by the file_path. Then it 
# inserts a string specified by the 'sentence' argument right before that found matched string. So in that case 
# we are finding the '</configuration>' string in the /home/${username}/hadoop/etc/hadoop/core-site.xml file 
# and inserting all the properties right before it.
sed -i "/<\/configuration>/i \
  <!-- URI of the HDFS NameNode -->\n\
  <property>\n\
    <name>fs.defaultFS</name>\n\
    <value>hdfs://$hadoop_master_hostname:9000</value>\n\
  </property>" /home/${username}/hadoop/etc/hadoop/core-site.xml


# === Section 8: Modify the hdfs-site.xml file ===

# add all the below properties to the configuration section of the hdfs-site.xml file
sed -i "/<\/configuration>/i \
  <!-- HDFS replication factor -->\n\
    <property>\n\
      <name>dfs.replication</name>\n\
      <value>2</value>\n\
    </property>\n\
  \n\
  <!-- DataNode directory. This is set up only on nodes (the Slave Node in this case) where we will be running DataNodes. -->\n\
  <property>\n\
    <name>dfs.datanode.data.dir</name>\n\
    <value>file:///home/${username}/hdfs/datanode</value>\n\
  </property>" /home/${username}/hadoop/etc/hadoop/hdfs-site.xml



# === Section 9: Modify the yarn-site.xml file ===

# add all the below properties to the configuration section of the yarn-site.xml file
sed -i "/<\/configuration>/i \n\
  <!-- Resource manager hostname -->\n\
  <property>\n\
      <name>yarn.resourcemanager.hostname</name>\n\
      <value>$hadoop_master_hostname</value>\n\
  </property>\n\
  \n\
  <!-- Define where local data should be stored for NodeManager -->\n\
  <property>\n\
      <name>yarn.nodemanager.local-dirs</name>\n\
      <value>/home/${username}/hadoop/yarn/local</value>\n\
  </property>\n\
  \n\
  <!-- Define log directory for containers -->\n\
  <property>\n\
      <name>yarn.nodemanager.log-dirs</name>\n\
      <value>/home/${username}/hadoop/yarn/logs</value>\n\
  </property>\n\
  \n\
  <!-- Use HDFS for auxiliary services like shuffle for MapReduce -->\n\
  <property>\n\
      <name>yarn.nodemanager.aux-services</name>\n\
      <value>mapreduce_shuffle</value>\n\
  </property>\n\
  \n\
  <!-- Auxiliary service class -->\n\
  <property>\n\
      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n\
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n\
  </property>" /home/${username}/hadoop/etc/hadoop/yarn-site.xml


# === Section 10: Create folders needed and grant permissions to the Hadoop user (which will be running Hadoop commands). ===

# Hadoop will be using the Hadoop user in order to modify files in the below folders. That's why we need to create them
# and grant proper permissions to that Hadoop user.
mkdir /home/${username}/hdfs
mkdir /home/${username}/hdfs/datanode # folder path specified in the hdfs-site.xml configuration
mkdir -p /home/${username}/hadoop/logs # folder needed for Hadoop logs

chown -R ${username}:${username} /home/${username}/hdfs
chmod -R 700 /home/${username}/hdfs
chown -R ${username}:${username} /home/${username}/hadoop