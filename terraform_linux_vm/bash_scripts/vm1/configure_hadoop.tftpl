#!/bin/bash

# That script will be executed on a VM as a root user. It will be at first rendered using the Terraform templatefile function.
# We are using here variables provided by that function:
# - username
# - hostnames
# - host_entries
# - ssh_private_key - SSH key used for connecting from VM1 to VM2 and from VM1 to the localhost.
# - ssh_public_key - SSH key used for connecting from VM1 to VM2 and from VM1 to the localhost.
# - jupyter_notebook_password - Password for accessing Jupyter Notebook. The default one is 'admin'.

# hostnames of both VMs
hostnames=( %{ for entry in hostnames ~} "${entry}" %{ endfor ~} )
hadoop_master_hostname=$${hostnames[0]}
hadoop_slave_hostname=$${hostnames[1]}


# === Section 1: Assign given hostnames to the private IP addresses of both VMs in the /etc/hosts file. ===

HOSTS_FILE="/etc/hosts"

echo "Adding entries to $HOSTS_FILE..."

# Entries to add to the /etc/hosts, that is lines mapping hostnames to IP addresses.
hosts_entries=( %{ for entry in host_entries ~} "${entry}" %{ endfor ~} )

for entry in "$${hosts_entries[@]}"; do
  # Check if entry already exists
  if grep -q "$entry" "$HOSTS_FILE"; then
    echo "Entry '$entry' already exists. Skipping."
  else
    # add entry to the hosts file
    echo "$entry" >> "$HOSTS_FILE"
    echo "Added: $entry"
  fi
done

echo "Done."



# === Section 2: Saving a SSH private key which will be used by Hadoop (and us) for connecting to the VM2 ===

mkdir -p /home/${username}/.ssh # Create the .ssh directory if it doesn't exists
chmod 700 /home/${username}/.ssh # Set correct permissions

# File to store the private key
KEY_FILE="/home/${username}/.ssh/id_rsa"

echo "Creating SSH key file..."
echo "${ssh_private_key}" >> "$KEY_FILE" # Save the SSH private key to the file

chmod 600 "$KEY_FILE" # Set correct permissions for that file.
chown -R ${username}:${username} /home/${username}/.ssh # Assign the ${username} user as the owner of the .ssh folder.

echo "SSH private key saved to $KEY_FILE with restricted permissions."


# === Section 3: Adding a SSH public key to authorized_keys so Hadoop can connect through SSH to the localhost ===
echo "${ssh_public_key}" >> /home/${username}/.ssh/authorized_keys


# === Section 4: Install Java ===

apt-get update
apt-get install openjdk-8-jdk -y


# === Section 5: Download Hadoop and Spark. Save both in the /home/${username}/hadoop and /home/${username}/spark respectively. ===

apt-get install curl
curl -L https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz | tar -xz -C /home/${username}
mv /home/${username}/hadoop-3.3.6 /home/${username}/hadoop

curl -L https://archive.apache.org/dist/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz | tar -xz -C /home/${username}
mv /home/${username}/spark-3.4.4-bin-hadoop3 /home/${username}/spark


# === Section 6: Modify the .bashrc file. We will define there environment variables needed for Hadoop. ===

# add all the below lines to the .bashrc file
cat << 'EOF' >> /home/${username}/.bashrc
export HADOOP_HOME="/home/${username}/hadoop"
export SPARK_HOME="/home/${username}/spark"
export HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop" # This is needed in order to use Spark shell using this command: 'spark-shell --master yarn'
# export YARN_CONF_DIR="
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export PATH=$PATH:$SPARK_HOME/bin
export MAPRED_HOME=$${HADOOP_HOME}
export HDFS_HOME=$${HADOOP_HOME}
EOF


# === Section 7: Modify the hadoop-env.sh, yarn-env.sh and spark-env.sh files. Add value for the JAVA_HOME environment variable. ===

echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" >> /home/${username}/hadoop/etc/hadoop/hadoop-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" >> /home/${username}/hadoop/etc/hadoop/yarn-env.sh

cp /home/${username}/spark/conf/spark-env.sh.template /home/${username}/spark/conf/spark-env.sh
echo "export SPARK_WORKER_CORES=12" >> /home/${username}/spark/conf/spark-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" >> /home/${username}/spark/conf/spark-env.sh


# === Section 8: Modify the core-site.xml file ===

# add all the below properties to the configuration section of the core-site.xml file

# We are using here syntax: 
# > sed -i '/pattern/i sentence' file_path
# We have here 3 arguments: 'pattern', 'sentence' and 'file_path'.
# It is finding a string matching the 'pattern' argument in the file at the path specified by the file_path. Then it 
# inserts a string specified by the 'sentence' argument right before that found matched string. So in that case 
# we are finding the '</configuration>' string in the /home/${username}/hadoop/etc/hadoop/core-site.xml file 
# and inserting all the properties right before it.
sed -i "/<\/configuration>/i \
  <!-- URI of the HDFS NameNode -->\n\
  <property>\n\
    <name>fs.defaultFS</name>\n\
    <value>hdfs://$hadoop_master_hostname:9000</value>\n\
  </property>" /home/${username}/hadoop/etc/hadoop/core-site.xml


# === Section 9: Modify the hdfs-site.xml file ===

# add all the below properties to the configuration section of the hdfs-site.xml file
sed -i "/<\/configuration>/i \
  <!-- HDFS replication factor -->\n\
    <property>\n\
      <name>dfs.replication</name>\n\
      <value>2</value>\n\
    </property>\n\
  \n\
  <!-- Namenode Directory. This is set up only on the node (Master Node in this case) where we will be running a NameNode. -->\n\
  <property>\n\
    <name>dfs.namenode.name.dir</name>\n\
    <value>file:///home/${username}/hdfs/namenode</value>\n\
  </property>" /home/${username}/hadoop/etc/hadoop/hdfs-site.xml



# === Section 10: Modify the yarn-site.xml file ===

# add all the below properties to the configuration section of the yarn-site.xml file
sed -i "/<\/configuration>/i \n\
  <!-- Resource manager hostname -->\n\
  <property>\n\
      <name>yarn.resourcemanager.hostname</name>\n\
      <value>$hadoop_master_hostname</value>\n\
  </property>\n\
  \n\
  <!-- Define where local data should be stored for NodeManager -->\n\
  <property>\n\
      <name>yarn.nodemanager.local-dirs</name>\n\
      <value>/home/${username}/hadoop/yarn/local</value>\n\
  </property>\n\
  \n\
  <!-- Define log directory for containers -->\n\
  <property>\n\
      <name>yarn.nodemanager.log-dirs</name>\n\
      <value>/home/${username}/hadoop/yarn/logs</value>\n\
  </property>\n\
  \n\
  <!-- Use HDFS for auxiliary services like shuffle for MapReduce -->\n\
  <property>\n\
      <name>yarn.nodemanager.aux-services</name>\n\
      <value>mapreduce_shuffle</value>\n\
  </property>\n\
  \n\
  <!-- Auxiliary service class -->\n\
  <property>\n\
      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n\
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n\
  </property>" /home/${username}/hadoop/etc/hadoop/yarn-site.xml


# === Section 11: Create the masters and workers files for Hadoop and Spark. ===

# Hadoop files
echo "$hadoop_master_hostname" > /home/${username}/hadoop/etc/hadoop/masters
echo "$hadoop_slave_hostname" > /home/${username}/hadoop/etc/hadoop/workers

# Spark file
cp /home/${username}/spark/conf/workers.template /home/${username}/spark/conf/workers
echo -e "\n$hadoop_slave_hostname" >> /home/${username}/spark/conf/workers


# === Section 12: Create needed folders and grant permissions for them to the Hadoop user (which will be running Hadoop commands). ===

# Hadoop will be using the Hadoop user in order to modify files in the below folders. That's why we need to create them
# and grant proper permissions to that Hadoop user.
mkdir -p /home/${username}/hadoop/logs # folder needed for Hadoop logs
mkdir -p /usr/local/hadoop
# Assign the $username as the owner of both folders so that user can create new folders there
chown -R ${username}:${username} /home/${username}/hadoop
chown -R ${username}:${username} /usr/local/hadoop


# === Section 13: Set up Jupyter Notebook. ===

apt install python3.10 # Install python
DEBIAN_FRONTEND=noninteractive apt install python3-pip -y # Install pip
# add /home/${username}/.local/bin to the path so we can use the pip command (pip is located in that folder).
echo 'export PATH=$PATH:/home/${username}/.local/bin' >> /home/${username}/.bashrc
PATH=$PATH:/home/${username}/.local/bin

pip install notebook # install Jupyter Notebook
pip install pyspark==3.4.4 findspark # install other needed libraries

# Set up a password to the Jupyter Notebook.

# generate the .jupyter/jupyter_notebook_config.py file. We need to run that command as the 'username' user in order to create that file
# in the /home/username folder.
sudo -u ${username} bash -c "jupyter notebook --generate-config" 

# Generate hashed password using the Jupyter 7+ module and the Terraform variable jupyter_notebook_password (default = 'admin').
HASHED_PASSWORD=$(python3 -c "from jupyter_server.auth import passwd; print(passwd('${jupyter_notebook_password}'))")

# Add the below lines to the jupyter_notebook_config.py file
cat << EOF >> /home/${username}/.jupyter/jupyter_notebook_config.py
c.NotebookApp.password = u'$HASHED_PASSWORD' # here we define our password to the Jupyter Notebook.
c.NotebookApp.open_browser = False
c.NotebookApp.ip = '0.0.0.0'
c.NotebookApp.port = 8888
EOF

# We need to define those variables in this shell session because here we are starting Jupyter Notebook.
SPARK_HOME=/home/${username}/spark
JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser # Start the Jupyter Notebook
